nohup: ignoring input
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          anomaly_detection   Is Training:        1                   
  Model ID:           PSM_GOracleAD       Model:              G_OracleAD          

[1mData Loader[0m
  Data:               PSM                 Root Path:          ./dataset/PSM       
  Data Path:          ETTh1.csv           Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mAnomaly Detection Task[0m
  Anomaly Ratio:      1.0                 

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             25                  Dec In:             7                   
  C Out:              25                  d model:            128                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       30                  Batch Size:         128                 
  Patience:           3                   Learning Rate:      0.0001              
  Des:                test                Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
lambda_grad: 0.2 use_grad_soft: 1
[Init] lambda_pred=0.1 lambda_grad=0.2 beta_grad=4.0 use_grad_soft=1 dist_norm=1 score_point=last grad_eps=1e-06
>>>>>>>start training : anomaly_detection_PSM_GOracleAD_G_OracleAD_PSM_ftM_sl100_ll48_pl0_dm128_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>
test: (87841, 25)
train: (132481, 25)
train 132382
test: (87841, 25)
train: (132481, 25)
val 26398
[Reg-Debug] |g| mean=2.166e-06 max=1.359e-04 | A mean=0.0400 std=0.0000
[Reg-Debug] |g| mean=3.044e-06 max=1.048e-04 | A mean=0.0400 std=0.0000
[Reg-Debug] |g| mean=2.611e-06 max=2.543e-05 | A mean=0.0400 std=0.0000
[Reg-Debug] |g| mean=2.648e-06 max=3.845e-05 | A mean=0.0400 std=0.0000
[Reg-Debug] |g| mean=2.405e-06 max=5.409e-05 | A mean=0.0400 std=0.0000
	iters: 100/1035, epoch: 1 | loss=0.547119 (base=0.539534, recon=0.477808, pred=0.617262, reg=0.037926, lambda_grad*reg=0.007585) | batch_x(mean/std)=-0.0711/1.0362 | speed=0.0869s/iter, left=81.3s
	iters: 200/1035, epoch: 1 | loss=0.274050 (base=0.243555, recon=0.220316, pred=0.232395, reg=0.152476, lambda_grad*reg=0.030495) | batch_x(mean/std)=-0.0993/0.9308 | speed=0.0725s/iter, left=60.6s
	iters: 300/1035, epoch: 1 | loss=0.302059 (base=0.274232, recon=0.246659, pred=0.275730, reg=0.139137, lambda_grad*reg=0.027827) | batch_x(mean/std)=0.0401/0.9749 | speed=0.0730s/iter, left=53.7s
	iters: 400/1035, epoch: 1 | loss=0.279815 (base=0.255737, recon=0.228947, pred=0.267897, reg=0.120387, lambda_grad*reg=0.024077) | batch_x(mean/std)=0.0164/0.9790 | speed=0.0727s/iter, left=46.2s
	iters: 500/1035, epoch: 1 | loss=0.379306 (base=0.357261, recon=0.322973, pred=0.342882, reg=0.110224, lambda_grad*reg=0.022045) | batch_x(mean/std)=0.0314/1.0621 | speed=0.0730s/iter, left=39.1s
	iters: 600/1035, epoch: 1 | loss=0.301229 (base=0.280611, recon=0.256195, pred=0.244163, reg=0.103089, lambda_grad*reg=0.020618) | batch_x(mean/std)=-0.0261/0.9631 | speed=0.0733s/iter, left=31.9s
	iters: 700/1035, epoch: 1 | loss=0.575275 (base=0.555578, recon=0.232384, pred=3.231948, reg=0.098483, lambda_grad*reg=0.019697) | batch_x(mean/std)=-0.0116/1.0117 | speed=0.0734s/iter, left=24.7s
	iters: 800/1035, epoch: 1 | loss=0.276858 (base=0.258933, recon=0.230199, pred=0.287333, reg=0.089626, lambda_grad*reg=0.017925) | batch_x(mean/std)=0.0039/1.0105 | speed=0.0737s/iter, left=17.4s
	iters: 900/1035, epoch: 1 | loss=0.290396 (base=0.273016, recon=0.243231, pred=0.297851, reg=0.086903, lambda_grad*reg=0.017381) | batch_x(mean/std)=0.1231/1.0596 | speed=0.0735s/iter, left=10.0s
	iters: 1000/1035, epoch: 1 | loss=0.191807 (base=0.178190, recon=0.159172, pred=0.190176, reg=0.068084, lambda_grad*reg=0.013617) | batch_x(mean/std)=-0.0032/0.9501 | speed=0.0740s/iter, left=2.7s
Epoch: 1 cost time: 83.1s | Train Loss: 0.364922 Vali Loss: 0.206669
Validation loss decreased (inf --> 0.206669).  Saving model ...
Updating learning rate to 0.0001
	iters: 100/1035, epoch: 2 | loss=0.220568 (base=0.208242, recon=0.181563, pred=0.266791, reg=0.061634, lambda_grad*reg=0.012327) | batch_x(mean/std)=0.0443/0.9894 | speed=0.1615s/iter, left=151.2s
	iters: 200/1035, epoch: 2 | loss=0.256121 (base=0.244239, recon=0.219974, pred=0.242648, reg=0.059412, lambda_grad*reg=0.011882) | batch_x(mean/std)=-0.0267/1.0610 | speed=0.0742s/iter, left=62.0s
	iters: 300/1035, epoch: 2 | loss=0.281579 (base=0.270604, recon=0.227712, pred=0.428915, reg=0.054877, lambda_grad*reg=0.010975) | batch_x(mean/std)=-0.0897/1.0123 | speed=0.0741s/iter, left=54.5s
	iters: 400/1035, epoch: 2 | loss=0.186483 (base=0.176612, recon=0.157209, pred=0.194037, reg=0.049354, lambda_grad*reg=0.009871) | batch_x(mean/std)=0.0095/1.0202 | speed=0.0743s/iter, left=47.2s
	iters: 500/1035, epoch: 2 | loss=0.202168 (base=0.192739, recon=0.172909, pred=0.198304, reg=0.047142, lambda_grad*reg=0.009428) | batch_x(mean/std)=0.0111/1.0879 | speed=0.0741s/iter, left=39.7s
	iters: 600/1035, epoch: 2 | loss=0.297117 (base=0.288074, recon=0.239131, pred=0.489430, reg=0.045216, lambda_grad*reg=0.009043) | batch_x(mean/std)=0.0539/1.1520 | speed=0.0743s/iter, left=32.4s
	iters: 700/1035, epoch: 2 | loss=0.185827 (base=0.177605, recon=0.160998, pred=0.166074, reg=0.041109, lambda_grad*reg=0.008222) | batch_x(mean/std)=-0.0286/1.0382 | speed=0.0745s/iter, left=25.0s
	iters: 800/1035, epoch: 2 | loss=0.179213 (base=0.172149, recon=0.157398, pred=0.147515, reg=0.035318, lambda_grad*reg=0.007064) | batch_x(mean/std)=-0.0431/0.9332 | speed=0.0742s/iter, left=17.5s
	iters: 900/1035, epoch: 2 | loss=0.302498 (base=0.294585, recon=0.264797, pred=0.297879, reg=0.039568, lambda_grad*reg=0.007914) | batch_x(mean/std)=-0.0045/1.1770 | speed=0.0739s/iter, left=10.1s
	iters: 1000/1035, epoch: 2 | loss=0.223015 (base=0.216041, recon=0.192752, pred=0.232888, reg=0.034870, lambda_grad*reg=0.006974) | batch_x(mean/std)=-0.0498/1.0568 | speed=0.0740s/iter, left=2.7s
Epoch: 2 cost time: 83.1s | Train Loss: 0.190543 Vali Loss: 0.168997
Validation loss decreased (0.206669 --> 0.168997).  Saving model ...
Updating learning rate to 5e-05
	iters: 100/1035, epoch: 3 | loss=0.192893 (base=0.186357, recon=0.164662, pred=0.216949, reg=0.032683, lambda_grad*reg=0.006537) | batch_x(mean/std)=-0.0348/1.0484 | speed=0.1620s/iter, left=151.6s
	iters: 200/1035, epoch: 3 | loss=0.162855 (base=0.156930, recon=0.131653, pred=0.252764, reg=0.029625, lambda_grad*reg=0.005925) | batch_x(mean/std)=-0.0112/0.9687 | speed=0.0743s/iter, left=62.1s
	iters: 300/1035, epoch: 3 | loss=0.165113 (base=0.159566, recon=0.136146, pred=0.234199, reg=0.027735, lambda_grad*reg=0.005547) | batch_x(mean/std)=-0.0861/0.9255 | speed=0.0744s/iter, left=54.8s
	iters: 400/1035, epoch: 3 | loss=0.191355 (base=0.185284, recon=0.167603, pred=0.176802, reg=0.030356, lambda_grad*reg=0.006071) | batch_x(mean/std)=0.0089/1.0036 | speed=0.0743s/iter, left=47.3s
	iters: 500/1035, epoch: 3 | loss=0.133260 (base=0.127921, recon=0.113582, pred=0.143390, reg=0.026697, lambda_grad*reg=0.005339) | batch_x(mean/std)=-0.0150/0.9095 | speed=0.0744s/iter, left=39.9s
	iters: 600/1035, epoch: 3 | loss=0.175876 (base=0.170196, recon=0.155740, pred=0.144557, reg=0.028403, lambda_grad*reg=0.005681) | batch_x(mean/std)=0.0212/1.0521 | speed=0.0743s/iter, left=32.4s
	iters: 700/1035, epoch: 3 | loss=0.225710 (base=0.219896, recon=0.195166, pred=0.247303, reg=0.029069, lambda_grad*reg=0.005814) | batch_x(mean/std)=0.0869/1.1188 | speed=0.0743s/iter, left=25.0s
	iters: 800/1035, epoch: 3 | loss=0.172091 (base=0.167001, recon=0.151625, pred=0.153759, reg=0.025450, lambda_grad*reg=0.005090) | batch_x(mean/std)=-0.0472/0.9457 | speed=0.0742s/iter, left=17.5s
	iters: 900/1035, epoch: 3 | loss=0.211972 (base=0.206396, recon=0.180487, pred=0.259086, reg=0.027879, lambda_grad*reg=0.005576) | batch_x(mean/std)=0.0346/1.0450 | speed=0.0744s/iter, left=10.1s
	iters: 1000/1035, epoch: 3 | loss=0.128993 (base=0.124495, recon=0.110628, pred=0.138676, reg=0.022487, lambda_grad*reg=0.004497) | batch_x(mean/std)=-0.0008/0.9619 | speed=0.0742s/iter, left=2.7s
Epoch: 3 cost time: 83.2s | Train Loss: 0.174741 Vali Loss: 0.163984
Validation loss decreased (0.168997 --> 0.163984).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100/1035, epoch: 4 | loss=0.199351 (base=0.194365, recon=0.175621, pred=0.187433, reg=0.024930, lambda_grad*reg=0.004986) | batch_x(mean/std)=-0.0064/1.0002 | speed=0.1622s/iter, left=151.8s
	iters: 200/1035, epoch: 4 | loss=0.153631 (base=0.148658, recon=0.134899, pred=0.137587, reg=0.024863, lambda_grad*reg=0.004973) | batch_x(mean/std)=0.0768/0.9581 | speed=0.0744s/iter, left=62.2s
	iters: 300/1035, epoch: 4 | loss=0.137387 (base=0.133020, recon=0.116350, pred=0.166700, reg=0.021833, lambda_grad*reg=0.004367) | batch_x(mean/std)=-0.0134/0.8990 | speed=0.0745s/iter, left=54.8s
	iters: 400/1035, epoch: 4 | loss=0.203135 (base=0.198691, recon=0.166135, pred=0.325567, reg=0.022220, lambda_grad*reg=0.004444) | batch_x(mean/std)=-0.0782/0.9574 | speed=0.0743s/iter, left=47.3s
	iters: 500/1035, epoch: 4 | loss=0.164806 (base=0.160220, recon=0.138157, pred=0.220636, reg=0.022929, lambda_grad*reg=0.004586) | batch_x(mean/std)=0.0247/0.9648 | speed=0.0742s/iter, left=39.8s
	iters: 600/1035, epoch: 4 | loss=0.157640 (base=0.153239, recon=0.138274, pred=0.149651, reg=0.022008, lambda_grad*reg=0.004402) | batch_x(mean/std)=-0.0115/0.9810 | speed=0.0744s/iter, left=32.4s
	iters: 700/1035, epoch: 4 | loss=0.187278 (base=0.182746, recon=0.158981, pred=0.237650, reg=0.022659, lambda_grad*reg=0.004532) | batch_x(mean/std)=-0.0227/0.9588 | speed=0.0740s/iter, left=24.9s
	iters: 800/1035, epoch: 4 | loss=0.148742 (base=0.144095, recon=0.127106, pred=0.169888, reg=0.023236, lambda_grad*reg=0.004647) | batch_x(mean/std)=0.0202/0.9826 | speed=0.0741s/iter, left=17.5s
	iters: 900/1035, epoch: 4 | loss=0.170174 (base=0.165700, recon=0.150654, pred=0.150465, reg=0.022371, lambda_grad*reg=0.004474) | batch_x(mean/std)=0.0359/0.9954 | speed=0.0741s/iter, left=10.1s
	iters: 1000/1035, epoch: 4 | loss=0.226069 (base=0.221539, recon=0.196755, pred=0.247840, reg=0.022649, lambda_grad*reg=0.004530) | batch_x(mean/std)=-0.0897/1.0888 | speed=0.0742s/iter, left=2.7s
Epoch: 4 cost time: 83.3s | Train Loss: 0.170666 Vali Loss: 0.162356
Validation loss decreased (0.163984 --> 0.162356).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100/1035, epoch: 5 | loss=0.198236 (base=0.194003, recon=0.174446, pred=0.195575, reg=0.021162, lambda_grad*reg=0.004232) | batch_x(mean/std)=-0.0391/1.0092 | speed=0.1629s/iter, left=152.5s
	iters: 200/1035, epoch: 5 | loss=0.176778 (base=0.172510, recon=0.142370, pred=0.301394, reg=0.021341, lambda_grad*reg=0.004268) | batch_x(mean/std)=0.0068/0.9783 | speed=0.0745s/iter, left=62.3s
	iters: 300/1035, epoch: 5 | loss=0.182953 (base=0.178767, recon=0.158240, pred=0.205268, reg=0.020934, lambda_grad*reg=0.004187) | batch_x(mean/std)=-0.0541/1.0317 | speed=0.0740s/iter, left=54.5s
	iters: 400/1035, epoch: 5 | loss=0.150205 (base=0.145989, recon=0.127592, pred=0.183975, reg=0.021079, lambda_grad*reg=0.004216) | batch_x(mean/std)=0.0093/0.9907 | speed=0.0744s/iter, left=47.3s
	iters: 500/1035, epoch: 5 | loss=0.154559 (base=0.150639, recon=0.133678, pred=0.169610, reg=0.019600, lambda_grad*reg=0.003920) | batch_x(mean/std)=-0.1179/0.9199 | speed=0.0743s/iter, left=39.8s
	iters: 600/1035, epoch: 5 | loss=0.136898 (base=0.132861, recon=0.118863, pred=0.139977, reg=0.020186, lambda_grad*reg=0.004037) | batch_x(mean/std)=-0.0254/0.9086 | speed=0.0744s/iter, left=32.4s
	iters: 700/1035, epoch: 5 | loss=0.190003 (base=0.185535, recon=0.169294, pred=0.162411, reg=0.022341, lambda_grad*reg=0.004468) | batch_x(mean/std)=0.0797/1.0648 | speed=0.0743s/iter, left=25.0s
	iters: 800/1035, epoch: 5 | loss=0.197426 (base=0.193374, recon=0.175385, pred=0.179891, reg=0.020260, lambda_grad*reg=0.004052) | batch_x(mean/std)=-0.0616/1.0411 | speed=0.0745s/iter, left=17.6s
	iters: 900/1035, epoch: 5 | loss=0.175773 (base=0.171525, recon=0.149957, pred=0.215687, reg=0.021239, lambda_grad*reg=0.004248) | batch_x(mean/std)=0.0305/1.0077 | speed=0.0743s/iter, left=10.1s
	iters: 1000/1035, epoch: 5 | loss=0.197560 (base=0.193404, recon=0.168745, pred=0.246587, reg=0.020778, lambda_grad*reg=0.004156) | batch_x(mean/std)=0.0452/1.0836 | speed=0.0747s/iter, left=2.7s
Epoch: 5 cost time: 83.5s | Train Loss: 0.167639 Vali Loss: 0.158931
Validation loss decreased (0.162356 --> 0.158931).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100/1035, epoch: 6 | loss=0.199930 (base=0.195814, recon=0.175414, pred=0.203993, reg=0.020583, lambda_grad*reg=0.004117) | batch_x(mean/std)=0.0186/1.0092 | speed=0.1635s/iter, left=153.0s
	iters: 200/1035, epoch: 6 | loss=0.124696 (base=0.120436, recon=0.108788, pred=0.116476, reg=0.021301, lambda_grad*reg=0.004260) | batch_x(mean/std)=-0.0171/0.9939 | speed=0.0745s/iter, left=62.3s
	iters: 300/1035, epoch: 6 | loss=0.266262 (base=0.261963, recon=0.226214, pred=0.357490, reg=0.021493, lambda_grad*reg=0.004299) | batch_x(mean/std)=-0.0601/1.1492 | speed=0.0741s/iter, left=54.6s
	iters: 400/1035, epoch: 6 | loss=0.177987 (base=0.173710, recon=0.152753, pred=0.209575, reg=0.021384, lambda_grad*reg=0.004277) | batch_x(mean/std)=0.0981/1.0673 | speed=0.0742s/iter, left=47.2s
	iters: 500/1035, epoch: 6 | loss=0.141878 (base=0.137976, recon=0.123545, pred=0.144307, reg=0.019513, lambda_grad*reg=0.003903) | batch_x(mean/std)=-0.0185/0.9482 | speed=0.0738s/iter, left=39.6s
	iters: 600/1035, epoch: 6 | loss=0.138006 (base=0.133878, recon=0.122183, pred=0.116956, reg=0.020641, lambda_grad*reg=0.004128) | batch_x(mean/std)=0.0483/0.9880 | speed=0.0746s/iter, left=32.5s
	iters: 700/1035, epoch: 6 | loss=0.155289 (base=0.151442, recon=0.134841, pred=0.166009, reg=0.019237, lambda_grad*reg=0.003847) | batch_x(mean/std)=-0.0460/0.9419 | speed=0.0747s/iter, left=25.1s
	iters: 800/1035, epoch: 6 | loss=0.137732 (base=0.133803, recon=0.110250, pred=0.235530, reg=0.019643, lambda_grad*reg=0.003929) | batch_x(mean/std)=-0.0072/0.9683 | speed=0.0758s/iter, left=17.9s
	iters: 900/1035, epoch: 6 | loss=0.147167 (base=0.143094, recon=0.128754, pred=0.143403, reg=0.020366, lambda_grad*reg=0.004073) | batch_x(mean/std)=-0.0671/0.9788 | speed=0.0755s/iter, left=10.3s
	iters: 1000/1035, epoch: 6 | loss=0.149873 (base=0.145696, recon=0.125163, pred=0.205327, reg=0.020884, lambda_grad*reg=0.004177) | batch_x(mean/std)=0.0260/1.0370 | speed=0.0752s/iter, left=2.7s
Epoch: 6 cost time: 84.3s | Train Loss: 0.165814 Vali Loss: 0.156893
Validation loss decreased (0.158931 --> 0.156893).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100/1035, epoch: 7 | loss=0.151981 (base=0.147895, recon=0.133720, pred=0.141744, reg=0.020432, lambda_grad*reg=0.004086) | batch_x(mean/std)=0.0633/0.9565 | speed=0.1699s/iter, left=159.1s
	iters: 200/1035, epoch: 7 | loss=0.123640 (base=0.119792, recon=0.108026, pred=0.117655, reg=0.019240, lambda_grad*reg=0.003848) | batch_x(mean/std)=-0.0447/0.9561 | speed=0.0753s/iter, left=63.0s
	iters: 300/1035, epoch: 7 | loss=0.165385 (base=0.161541, recon=0.137096, pred=0.244452, reg=0.019221, lambda_grad*reg=0.003844) | batch_x(mean/std)=-0.0429/0.9932 | speed=0.0757s/iter, left=55.7s
	iters: 400/1035, epoch: 7 | loss=0.159413 (base=0.155536, recon=0.132657, pred=0.228794, reg=0.019386, lambda_grad*reg=0.003877) | batch_x(mean/std)=-0.0616/0.9307 | speed=0.0748s/iter, left=47.6s
	iters: 500/1035, epoch: 7 | loss=0.161944 (base=0.157929, recon=0.118154, pred=0.397749, reg=0.020078, lambda_grad*reg=0.004016) | batch_x(mean/std)=-0.0680/0.9041 | speed=0.0753s/iter, left=40.3s
	iters: 600/1035, epoch: 7 | loss=0.174294 (base=0.169977, recon=0.154979, pred=0.149983, reg=0.021582, lambda_grad*reg=0.004316) | batch_x(mean/std)=0.1245/1.0926 | speed=0.0757s/iter, left=33.0s
	iters: 700/1035, epoch: 7 | loss=0.175312 (base=0.171462, recon=0.155121, pred=0.163409, reg=0.019249, lambda_grad*reg=0.003850) | batch_x(mean/std)=-0.0653/0.9664 | speed=0.0752s/iter, left=25.3s
	iters: 800/1035, epoch: 7 | loss=0.214546 (base=0.210656, recon=0.193108, pred=0.175479, reg=0.019452, lambda_grad*reg=0.003890) | batch_x(mean/std)=-0.0095/0.9854 | speed=0.0757s/iter, left=17.9s
	iters: 900/1035, epoch: 7 | loss=0.192647 (base=0.188725, recon=0.149129, pred=0.395958, reg=0.019610, lambda_grad*reg=0.003922) | batch_x(mean/std)=-0.0481/0.9709 | speed=0.0755s/iter, left=10.3s
	iters: 1000/1035, epoch: 7 | loss=0.191107 (base=0.186993, recon=0.170703, pred=0.162897, reg=0.020569, lambda_grad*reg=0.004114) | batch_x(mean/std)=-0.0230/1.1029 | speed=0.0751s/iter, left=2.7s
Epoch: 7 cost time: 85.0s | Train Loss: 0.165107 Vali Loss: 0.156362
Validation loss decreased (0.156893 --> 0.156362).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100/1035, epoch: 8 | loss=0.119401 (base=0.115553, recon=0.103366, pred=0.121873, reg=0.019241, lambda_grad*reg=0.003848) | batch_x(mean/std)=-0.0124/0.9321 | speed=0.1692s/iter, left=158.3s
	iters: 200/1035, epoch: 8 | loss=0.171788 (base=0.167710, recon=0.152579, pred=0.151308, reg=0.020391, lambda_grad*reg=0.004078) | batch_x(mean/std)=-0.0243/0.9710 | speed=0.0753s/iter, left=63.0s
	iters: 300/1035, epoch: 8 | loss=0.116631 (base=0.112552, recon=0.097983, pred=0.145693, reg=0.020395, lambda_grad*reg=0.004079) | batch_x(mean/std)=0.0675/1.0174 | speed=0.0757s/iter, left=55.7s
	iters: 400/1035, epoch: 8 | loss=0.208711 (base=0.204524, recon=0.190914, pred=0.136106, reg=0.020935, lambda_grad*reg=0.004187) | batch_x(mean/std)=0.1278/1.0522 | speed=0.0753s/iter, left=47.9s
	iters: 500/1035, epoch: 8 | loss=0.235086 (base=0.230765, recon=0.208574, pred=0.221910, reg=0.021602, lambda_grad*reg=0.004320) | batch_x(mean/std)=0.1195/1.1641 | speed=0.0756s/iter, left=40.5s
	iters: 600/1035, epoch: 8 | loss=0.214216 (base=0.210077, recon=0.181127, pred=0.289510, reg=0.020693, lambda_grad*reg=0.004139) | batch_x(mean/std)=-0.0686/1.0289 | speed=0.0753s/iter, left=32.8s
	iters: 700/1035, epoch: 8 | loss=0.176899 (base=0.172886, recon=0.153350, pred=0.195358, reg=0.020063, lambda_grad*reg=0.004013) | batch_x(mean/std)=-0.0730/0.9944 | speed=0.0755s/iter, left=25.4s
	iters: 800/1035, epoch: 8 | loss=0.190316 (base=0.186234, recon=0.154774, pred=0.314594, reg=0.020410, lambda_grad*reg=0.004082) | batch_x(mean/std)=-0.0620/0.9953 | speed=0.0756s/iter, left=17.8s
	iters: 900/1035, epoch: 8 | loss=0.155272 (base=0.151065, recon=0.133757, pred=0.173078, reg=0.021032, lambda_grad*reg=0.004206) | batch_x(mean/std)=0.0901/1.0538 | speed=0.0755s/iter, left=10.3s
	iters: 1000/1035, epoch: 8 | loss=0.200443 (base=0.196491, recon=0.175813, pred=0.206778, reg=0.019760, lambda_grad*reg=0.003952) | batch_x(mean/std)=-0.0718/1.0139 | speed=0.0754s/iter, left=2.7s
Epoch: 8 cost time: 85.4s | Train Loss: 0.164831 Vali Loss: 0.155939
Validation loss decreased (0.156362 --> 0.155939).  Saving model ...
Updating learning rate to 7.8125e-07
	iters: 100/1035, epoch: 9 | loss=0.152252 (base=0.148291, recon=0.131587, pred=0.167043, reg=0.019807, lambda_grad*reg=0.003961) | batch_x(mean/std)=0.0147/0.9560 | speed=0.1707s/iter, left=159.8s
	iters: 200/1035, epoch: 9 | loss=0.146011 (base=0.141856, recon=0.121621, pred=0.202351, reg=0.020775, lambda_grad*reg=0.004155) | batch_x(mean/std)=0.0999/1.0063 | speed=0.0759s/iter, left=63.5s
	iters: 300/1035, epoch: 9 | loss=0.266037 (base=0.261845, recon=0.232352, pred=0.294936, reg=0.020959, lambda_grad*reg=0.004192) | batch_x(mean/std)=-0.1222/1.0855 | speed=0.0759s/iter, left=55.8s
	iters: 400/1035, epoch: 9 | loss=0.196723 (base=0.192623, recon=0.176826, pred=0.157977, reg=0.020498, lambda_grad*reg=0.004100) | batch_x(mean/std)=0.0016/1.0323 | speed=0.0763s/iter, left=48.5s
	iters: 500/1035, epoch: 9 | loss=0.128009 (base=0.124141, recon=0.111755, pred=0.123868, reg=0.019340, lambda_grad*reg=0.003868) | batch_x(mean/std)=-0.0739/0.9260 | speed=0.0761s/iter, left=40.8s
	iters: 600/1035, epoch: 9 | loss=0.182380 (base=0.178561, recon=0.158553, pred=0.200083, reg=0.019095, lambda_grad*reg=0.003819) | batch_x(mean/std)=-0.0555/0.9561 | speed=0.0762s/iter, left=33.2s
	iters: 700/1035, epoch: 9 | loss=0.162149 (base=0.158100, recon=0.142066, pred=0.160342, reg=0.020244, lambda_grad*reg=0.004049) | batch_x(mean/std)=0.0911/1.0355 | speed=0.0765s/iter, left=25.7s
	iters: 800/1035, epoch: 9 | loss=0.118803 (base=0.114724, recon=0.101915, pred=0.128089, reg=0.020392, lambda_grad*reg=0.004078) | batch_x(mean/std)=0.0844/0.9801 | speed=0.0763s/iter, left=18.0s
	iters: 900/1035, epoch: 9 | loss=0.158697 (base=0.154687, recon=0.141785, pred=0.129020, reg=0.020047, lambda_grad*reg=0.004009) | batch_x(mean/std)=-0.0093/0.9268 | speed=0.0758s/iter, left=10.3s
	iters: 1000/1035, epoch: 9 | loss=0.200039 (base=0.195792, recon=0.176642, pred=0.191497, reg=0.021236, lambda_grad*reg=0.004247) | batch_x(mean/std)=0.0471/1.0947 | speed=0.0759s/iter, left=2.7s
Epoch: 9 cost time: 85.8s | Train Loss: 0.164269 Vali Loss: 0.155819
Validation loss decreased (0.155939 --> 0.155819).  Saving model ...
Updating learning rate to 3.90625e-07
	iters: 100/1035, epoch: 10 | loss=0.206595 (base=0.202583, recon=0.186034, pred=0.165489, reg=0.020059, lambda_grad*reg=0.004012) | batch_x(mean/std)=0.0624/1.0098 | speed=0.1701s/iter, left=159.2s
	iters: 200/1035, epoch: 10 | loss=0.192894 (base=0.188757, recon=0.170843, pred=0.179144, reg=0.020684, lambda_grad*reg=0.004137) | batch_x(mean/std)=-0.0152/1.0402 | speed=0.0763s/iter, left=63.8s
	iters: 300/1035, epoch: 10 | loss=0.165024 (base=0.160904, recon=0.145458, pred=0.154463, reg=0.020599, lambda_grad*reg=0.004120) | batch_x(mean/std)=0.0730/1.0460 | speed=0.0764s/iter, left=56.2s
	iters: 400/1035, epoch: 10 | loss=0.124426 (base=0.120316, recon=0.107439, pred=0.128765, reg=0.020551, lambda_grad*reg=0.004110) | batch_x(mean/std)=-0.0189/0.9546 | speed=0.0764s/iter, left=48.6s
	iters: 500/1035, epoch: 10 | loss=0.204512 (base=0.200588, recon=0.183305, pred=0.172833, reg=0.019619, lambda_grad*reg=0.003924) | batch_x(mean/std)=-0.0892/0.9974 | speed=0.0763s/iter, left=40.9s
	iters: 600/1035, epoch: 10 | loss=0.205165 (base=0.200848, recon=0.179418, pred=0.214293, reg=0.021589, lambda_grad*reg=0.004318) | batch_x(mean/std)=0.0422/1.0703 | speed=0.0757s/iter, left=33.0s
	iters: 700/1035, epoch: 10 | loss=0.131868 (base=0.127832, recon=0.114820, pred=0.130112, reg=0.020181, lambda_grad*reg=0.004036) | batch_x(mean/std)=-0.0339/0.9656 | speed=0.0763s/iter, left=25.6s
	iters: 800/1035, epoch: 10 | loss=0.162787 (base=0.159117, recon=0.144325, pred=0.147927, reg=0.018350, lambda_grad*reg=0.003670) | batch_x(mean/std)=-0.0204/0.9718 | speed=0.0763s/iter, left=18.0s
	iters: 900/1035, epoch: 10 | loss=0.114437 (base=0.110494, recon=0.097357, pred=0.131374, reg=0.019713, lambda_grad*reg=0.003943) | batch_x(mean/std)=0.1003/0.9920 | speed=0.0757s/iter, left=10.3s
	iters: 1000/1035, epoch: 10 | loss=0.148166 (base=0.144315, recon=0.123928, pred=0.203871, reg=0.019256, lambda_grad*reg=0.003851) | batch_x(mean/std)=0.1020/0.9524 | speed=0.0751s/iter, left=2.7s
Epoch: 10 cost time: 85.4s | Train Loss: 0.164392 Vali Loss: 0.155507
Validation loss decreased (0.155819 --> 0.155507).  Saving model ...
Updating learning rate to 1.953125e-07
	iters: 100/1035, epoch: 11 | loss=0.149091 (base=0.145169, recon=0.132109, pred=0.130598, reg=0.019610, lambda_grad*reg=0.003922) | batch_x(mean/std)=-0.0656/0.9881 | speed=0.1680s/iter, left=157.3s
	iters: 200/1035, epoch: 11 | loss=0.169345 (base=0.165313, recon=0.151263, pred=0.140502, reg=0.020161, lambda_grad*reg=0.004032) | batch_x(mean/std)=0.0214/0.9751 | speed=0.0756s/iter, left=63.2s
	iters: 300/1035, epoch: 11 | loss=0.157959 (base=0.154002, recon=0.134706, pred=0.192958, reg=0.019784, lambda_grad*reg=0.003957) | batch_x(mean/std)=0.0073/0.9680 | speed=0.0762s/iter, left=56.1s
	iters: 400/1035, epoch: 11 | loss=0.165854 (base=0.161819, recon=0.145783, pred=0.160365, reg=0.020175, lambda_grad*reg=0.004035) | batch_x(mean/std)=-0.0093/0.9952 | speed=0.0758s/iter, left=48.2s
	iters: 500/1035, epoch: 11 | loss=0.232806 (base=0.228734, recon=0.209509, pred=0.192247, reg=0.020357, lambda_grad*reg=0.004071) | batch_x(mean/std)=0.0358/1.0086 | speed=0.0760s/iter, left=40.7s
	iters: 600/1035, epoch: 11 | loss=0.180548 (base=0.176714, recon=0.150302, pred=0.264112, reg=0.019174, lambda_grad*reg=0.003835) | batch_x(mean/std)=-0.0240/0.9402 | speed=0.0753s/iter, left=32.8s
	iters: 700/1035, epoch: 11 | loss=0.192571 (base=0.188751, recon=0.170368, pred=0.183830, reg=0.019101, lambda_grad*reg=0.003820) | batch_x(mean/std)=0.0597/0.9610 | speed=0.0756s/iter, left=25.4s
	iters: 800/1035, epoch: 11 | loss=0.138626 (base=0.134815, recon=0.117640, pred=0.171756, reg=0.019053, lambda_grad*reg=0.003811) | batch_x(mean/std)=-0.1660/0.9507 | speed=0.0756s/iter, left=17.8s
	iters: 900/1035, epoch: 11 | loss=0.197568 (base=0.193796, recon=0.165289, pred=0.285071, reg=0.018860, lambda_grad*reg=0.003772) | batch_x(mean/std)=0.0160/0.9626 | speed=0.0754s/iter, left=10.3s
	iters: 1000/1035, epoch: 11 | loss=0.124050 (base=0.120067, recon=0.108454, pred=0.116130, reg=0.019913, lambda_grad*reg=0.003983) | batch_x(mean/std)=0.0405/0.9715 | speed=0.0753s/iter, left=2.7s
Epoch: 11 cost time: 85.2s | Train Loss: 0.164191 Vali Loss: 0.155492
Validation loss decreased (0.155507 --> 0.155492).  Saving model ...
Updating learning rate to 9.765625e-08
	iters: 100/1035, epoch: 12 | loss=0.239971 (base=0.235742, recon=0.211682, pred=0.240602, reg=0.021144, lambda_grad*reg=0.004229) | batch_x(mean/std)=0.0039/1.1201 | speed=0.1679s/iter, left=157.2s
	iters: 200/1035, epoch: 12 | loss=0.134021 (base=0.130198, recon=0.117563, pred=0.126352, reg=0.019112, lambda_grad*reg=0.003822) | batch_x(mean/std)=-0.0081/0.9519 | speed=0.0757s/iter, left=63.3s
	iters: 300/1035, epoch: 12 | loss=0.130621 (base=0.126464, recon=0.115176, pred=0.112883, reg=0.020782, lambda_grad*reg=0.004156) | batch_x(mean/std)=0.0736/0.9839 | speed=0.0757s/iter, left=55.7s
	iters: 400/1035, epoch: 12 | loss=0.144097 (base=0.140320, recon=0.125797, pred=0.145225, reg=0.018884, lambda_grad*reg=0.003777) | batch_x(mean/std)=-0.0611/0.9714 | speed=0.0759s/iter, left=48.3s
	iters: 500/1035, epoch: 12 | loss=0.133285 (base=0.129352, recon=0.116823, pred=0.125295, reg=0.019665, lambda_grad*reg=0.003933) | batch_x(mean/std)=0.0046/0.9602 | speed=0.0758s/iter, left=40.6s
	iters: 600/1035, epoch: 12 | loss=0.197000 (base=0.192796, recon=0.171382, pred=0.214139, reg=0.021024, lambda_grad*reg=0.004205) | batch_x(mean/std)=0.0368/1.0120 | speed=0.0764s/iter, left=33.3s
	iters: 700/1035, epoch: 12 | loss=0.213446 (base=0.209251, recon=0.183244, pred=0.260068, reg=0.020975, lambda_grad*reg=0.004195) | batch_x(mean/std)=0.1171/1.1476 | speed=0.0755s/iter, left=25.4s
	iters: 800/1035, epoch: 12 | loss=0.147194 (base=0.143247, recon=0.130908, pred=0.123388, reg=0.019732, lambda_grad*reg=0.003946) | batch_x(mean/std)=-0.0571/0.9743 | speed=0.0756s/iter, left=17.8s
	iters: 900/1035, epoch: 12 | loss=0.114423 (base=0.110320, recon=0.098653, pred=0.116667, reg=0.020516, lambda_grad*reg=0.004103) | batch_x(mean/std)=0.1206/0.9850 | speed=0.0754s/iter, left=10.2s
	iters: 1000/1035, epoch: 12 | loss=0.282876 (base=0.278891, recon=0.248068, pred=0.308232, reg=0.019923, lambda_grad*reg=0.003985) | batch_x(mean/std)=-0.0466/1.0616 | speed=0.0756s/iter, left=2.7s
Epoch: 12 cost time: 85.5s | Train Loss: 0.163786 Vali Loss: 0.155549
EarlyStopping counter: 1 out of 3
Updating learning rate to 4.8828125e-08
	iters: 100/1035, epoch: 13 | loss=0.150386 (base=0.146126, recon=0.128599, pred=0.175265, reg=0.021302, lambda_grad*reg=0.004260) | batch_x(mean/std)=0.0835/1.0305 | speed=0.1687s/iter, left=157.9s
	iters: 200/1035, epoch: 13 | loss=0.167698 (base=0.163738, recon=0.146583, pred=0.171555, reg=0.019797, lambda_grad*reg=0.003959) | batch_x(mean/std)=-0.0516/1.0074 | speed=0.0752s/iter, left=62.9s
	iters: 300/1035, epoch: 13 | loss=0.174296 (base=0.170254, recon=0.136037, pred=0.342170, reg=0.020209, lambda_grad*reg=0.004042) | batch_x(mean/std)=0.0131/1.0200 | speed=0.0752s/iter, left=55.4s
	iters: 400/1035, epoch: 13 | loss=0.188362 (base=0.184271, recon=0.163579, pred=0.206916, reg=0.020455, lambda_grad*reg=0.004091) | batch_x(mean/std)=0.0525/1.0527 | speed=0.0754s/iter, left=48.0s
	iters: 500/1035, epoch: 13 | loss=0.134612 (base=0.130770, recon=0.114477, pred=0.162932, reg=0.019207, lambda_grad*reg=0.003841) | batch_x(mean/std)=-0.0631/0.9517 | speed=0.0757s/iter, left=40.6s
	iters: 600/1035, epoch: 13 | loss=0.189196 (base=0.184939, recon=0.165405, pred=0.195337, reg=0.021288, lambda_grad*reg=0.004258) | batch_x(mean/std)=0.0189/1.1376 | speed=0.0755s/iter, left=32.9s
	iters: 700/1035, epoch: 13 | loss=0.208311 (base=0.204467, recon=0.182694, pred=0.217731, reg=0.019221, lambda_grad*reg=0.003844) | batch_x(mean/std)=-0.0057/0.9942 | speed=0.0756s/iter, left=25.4s
	iters: 800/1035, epoch: 13 | loss=0.150935 (base=0.146977, recon=0.131009, pred=0.159677, reg=0.019789, lambda_grad*reg=0.003958) | batch_x(mean/std)=-0.0132/0.9875 | speed=0.0761s/iter, left=18.0s
	iters: 900/1035, epoch: 13 | loss=0.139675 (base=0.135617, recon=0.123116, pred=0.125010, reg=0.020289, lambda_grad*reg=0.004058) | batch_x(mean/std)=0.0128/0.9954 | speed=0.0758s/iter, left=10.3s
	iters: 1000/1035, epoch: 13 | loss=0.296490 (base=0.291941, recon=0.258557, pred=0.333835, reg=0.022745, lambda_grad*reg=0.004549) | batch_x(mean/std)=0.1179/1.1821 | speed=0.0762s/iter, left=2.7s
Epoch: 13 cost time: 85.5s | Train Loss: 0.164093 Vali Loss: 0.155477
Validation loss decreased (0.155492 --> 0.155477).  Saving model ...
Updating learning rate to 2.44140625e-08
	iters: 100/1035, epoch: 14 | loss=0.155831 (base=0.151809, recon=0.136894, pred=0.149147, reg=0.020108, lambda_grad*reg=0.004022) | batch_x(mean/std)=0.0474/1.0462 | speed=0.1713s/iter, left=160.3s
	iters: 200/1035, epoch: 14 | loss=0.148324 (base=0.144298, recon=0.122654, pred=0.216445, reg=0.020127, lambda_grad*reg=0.004025) | batch_x(mean/std)=0.0459/1.0048 | speed=0.0753s/iter, left=62.9s
	iters: 300/1035, epoch: 14 | loss=0.132466 (base=0.128593, recon=0.115854, pred=0.127398, reg=0.019365, lambda_grad*reg=0.003873) | batch_x(mean/std)=0.0019/0.9610 | speed=0.0751s/iter, left=55.3s
	iters: 400/1035, epoch: 14 | loss=0.151747 (base=0.147651, recon=0.130641, pred=0.170098, reg=0.020481, lambda_grad*reg=0.004096) | batch_x(mean/std)=0.0598/1.0308 | speed=0.0756s/iter, left=48.1s
	iters: 500/1035, epoch: 14 | loss=0.150524 (base=0.146518, recon=0.127724, pred=0.187939, reg=0.020033, lambda_grad*reg=0.004007) | batch_x(mean/std)=0.0108/1.0349 | speed=0.0760s/iter, left=40.8s
	iters: 600/1035, epoch: 14 | loss=0.190758 (base=0.186746, recon=0.153598, pred=0.331474, reg=0.020063, lambda_grad*reg=0.004013) | batch_x(mean/std)=-0.0226/0.9893 | speed=0.0753s/iter, left=32.8s
	iters: 700/1035, epoch: 14 | loss=0.217612 (base=0.213550, recon=0.183171, pred=0.303782, reg=0.020311, lambda_grad*reg=0.004062) | batch_x(mean/std)=0.0028/1.0715 | speed=0.0757s/iter, left=25.4s
	iters: 800/1035, epoch: 14 | loss=0.169732 (base=0.165343, recon=0.150359, pred=0.149840, reg=0.021948, lambda_grad*reg=0.004390) | batch_x(mean/std)=0.0899/1.0387 | speed=0.0758s/iter, left=17.9s
	iters: 900/1035, epoch: 14 | loss=0.134444 (base=0.130737, recon=0.115834, pred=0.149022, reg=0.018538, lambda_grad*reg=0.003708) | batch_x(mean/std)=-0.0178/0.9418 | speed=0.0755s/iter, left=10.3s
	iters: 1000/1035, epoch: 14 | loss=0.210260 (base=0.206156, recon=0.178153, pred=0.280030, reg=0.020521, lambda_grad*reg=0.004104) | batch_x(mean/std)=-0.0105/1.0335 | speed=0.0754s/iter, left=2.7s
Epoch: 14 cost time: 85.5s | Train Loss: 0.164007 Vali Loss: 0.155566
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.220703125e-08
	iters: 100/1035, epoch: 15 | loss=0.156250 (base=0.152386, recon=0.136845, pred=0.155416, reg=0.019318, lambda_grad*reg=0.003864) | batch_x(mean/std)=-0.1928/0.9357 | speed=0.1703s/iter, left=159.4s
	iters: 200/1035, epoch: 15 | loss=0.169297 (base=0.165087, recon=0.148240, pred=0.168472, reg=0.021049, lambda_grad*reg=0.004210) | batch_x(mean/std)=0.0844/1.0601 | speed=0.0752s/iter, left=62.9s
	iters: 300/1035, epoch: 15 | loss=0.131722 (base=0.127817, recon=0.114019, pred=0.137975, reg=0.019525, lambda_grad*reg=0.003905) | batch_x(mean/std)=-0.0308/0.9580 | speed=0.0761s/iter, left=56.0s
	iters: 400/1035, epoch: 15 | loss=0.133745 (base=0.129781, recon=0.116233, pred=0.135487, reg=0.019816, lambda_grad*reg=0.003963) | batch_x(mean/std)=-0.0199/0.9957 | speed=0.0751s/iter, left=47.7s
	iters: 500/1035, epoch: 15 | loss=0.178256 (base=0.174006, recon=0.152405, pred=0.216008, reg=0.021252, lambda_grad*reg=0.004250) | batch_x(mean/std)=0.0905/1.0331 | speed=0.0752s/iter, left=40.3s
	iters: 600/1035, epoch: 15 | loss=0.224814 (base=0.220760, recon=0.199166, pred=0.215937, reg=0.020272, lambda_grad*reg=0.004054) | batch_x(mean/std)=-0.0115/1.0254 | speed=0.0753s/iter, left=32.8s
	iters: 700/1035, epoch: 15 | loss=0.125356 (base=0.121508, recon=0.110051, pred=0.114569, reg=0.019236, lambda_grad*reg=0.003847) | batch_x(mean/std)=0.0216/0.8995 | speed=0.0753s/iter, left=25.3s
	iters: 800/1035, epoch: 15 | loss=0.188392 (base=0.184521, recon=0.150353, pred=0.341674, reg=0.019356, lambda_grad*reg=0.003871) | batch_x(mean/std)=-0.1248/0.9926 | speed=0.0758s/iter, left=17.9s
	iters: 900/1035, epoch: 15 | loss=0.174210 (base=0.170199, recon=0.145113, pred=0.250866, reg=0.020054, lambda_grad*reg=0.004011) | batch_x(mean/std)=0.0436/1.0288 | speed=0.0753s/iter, left=10.2s
	iters: 1000/1035, epoch: 15 | loss=0.130841 (base=0.126701, recon=0.112531, pred=0.141700, reg=0.020699, lambda_grad*reg=0.004140) | batch_x(mean/std)=0.1117/1.0325 | speed=0.0756s/iter, left=2.7s
Epoch: 15 cost time: 85.2s | Train Loss: 0.164140 Vali Loss: 0.155461
Validation loss decreased (0.155477 --> 0.155461).  Saving model ...
Updating learning rate to 6.103515625e-09
	iters: 100/1035, epoch: 16 | loss=0.188071 (base=0.184240, recon=0.162322, pred=0.219177, reg=0.019159, lambda_grad*reg=0.003832) | batch_x(mean/std)=-0.0500/0.9990 | speed=0.1715s/iter, left=160.5s
	iters: 200/1035, epoch: 16 | loss=0.154942 (base=0.151010, recon=0.135054, pred=0.159557, reg=0.019663, lambda_grad*reg=0.003933) | batch_x(mean/std)=0.0049/0.9959 | speed=0.0753s/iter, left=62.9s
	iters: 300/1035, epoch: 16 | loss=0.175354 (base=0.171562, recon=0.149827, pred=0.217350, reg=0.018957, lambda_grad*reg=0.003791) | batch_x(mean/std)=-0.0682/0.9789 | speed=0.0752s/iter, left=55.3s
	iters: 400/1035, epoch: 16 | loss=0.221326 (base=0.217292, recon=0.185335, pred=0.319560, reg=0.020175, lambda_grad*reg=0.004035) | batch_x(mean/std)=0.0350/0.9965 | speed=0.0754s/iter, left=48.0s
	iters: 500/1035, epoch: 16 | loss=0.147575 (base=0.143698, recon=0.130516, pred=0.131820, reg=0.019383, lambda_grad*reg=0.003877) | batch_x(mean/std)=-0.0080/0.9950 | speed=0.0754s/iter, left=40.4s
	iters: 600/1035, epoch: 16 | loss=0.256639 (base=0.252693, recon=0.217483, pred=0.352101, reg=0.019726, lambda_grad*reg=0.003945) | batch_x(mean/std)=-0.0401/0.9893 | speed=0.0752s/iter, left=32.8s
	iters: 700/1035, epoch: 16 | loss=0.342693 (base=0.338567, recon=0.200481, pred=1.380865, reg=0.020628, lambda_grad*reg=0.004126) | batch_x(mean/std)=-0.0910/1.0770 | speed=0.0758s/iter, left=25.5s
	iters: 800/1035, epoch: 16 | loss=0.164056 (base=0.159825, recon=0.144199, pred=0.156256, reg=0.021157, lambda_grad*reg=0.004231) | batch_x(mean/std)=0.0099/1.0209 | speed=0.0755s/iter, left=17.8s
	iters: 900/1035, epoch: 16 | loss=0.152636 (base=0.148626, recon=0.126889, pred=0.217371, reg=0.020052, lambda_grad*reg=0.004010) | batch_x(mean/std)=0.0458/0.9586 | speed=0.0752s/iter, left=10.2s
	iters: 1000/1035, epoch: 16 | loss=0.184213 (base=0.180230, recon=0.161630, pred=0.185995, reg=0.019913, lambda_grad*reg=0.003983) | batch_x(mean/std)=0.0443/1.0295 | speed=0.0757s/iter, left=2.7s
Epoch: 16 cost time: 85.3s | Train Loss: 0.164069 Vali Loss: 0.155499
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.0517578125e-09
	iters: 100/1035, epoch: 17 | loss=0.139617 (base=0.135727, recon=0.120874, pred=0.148535, reg=0.019446, lambda_grad*reg=0.003889) | batch_x(mean/std)=0.0020/0.9413 | speed=0.1692s/iter, left=158.4s
	iters: 200/1035, epoch: 17 | loss=0.227278 (base=0.222857, recon=0.195230, pred=0.276273, reg=0.022107, lambda_grad*reg=0.004421) | batch_x(mean/std)=0.0039/1.1298 | speed=0.0754s/iter, left=63.1s
	iters: 300/1035, epoch: 17 | loss=0.166312 (base=0.162531, recon=0.141898, pred=0.206324, reg=0.018904, lambda_grad*reg=0.003781) | batch_x(mean/std)=-0.1194/0.9627 | speed=0.0751s/iter, left=55.3s
	iters: 400/1035, epoch: 17 | loss=0.196160 (base=0.191994, recon=0.160049, pred=0.319456, reg=0.020829, lambda_grad*reg=0.004166) | batch_x(mean/std)=0.0297/1.0111 | speed=0.0756s/iter, left=48.1s
	iters: 500/1035, epoch: 17 | loss=0.127657 (base=0.123591, recon=0.109901, pred=0.136901, reg=0.020329, lambda_grad*reg=0.004066) | batch_x(mean/std)=0.0772/1.0107 | speed=0.0755s/iter, left=40.5s
	iters: 600/1035, epoch: 17 | loss=0.146565 (base=0.142419, recon=0.124349, pred=0.180694, reg=0.020732, lambda_grad*reg=0.004146) | batch_x(mean/std)=-0.0982/1.0079 | speed=0.0753s/iter, left=32.8s
	iters: 700/1035, epoch: 17 | loss=0.124176 (base=0.120078, recon=0.105844, pred=0.142340, reg=0.020493, lambda_grad*reg=0.004099) | batch_x(mean/std)=0.0280/1.0140 | speed=0.0754s/iter, left=25.3s
	iters: 800/1035, epoch: 17 | loss=0.169649 (base=0.165413, recon=0.149689, pred=0.157242, reg=0.021179, lambda_grad*reg=0.004236) | batch_x(mean/std)=0.0376/1.0464 | speed=0.0750s/iter, left=17.7s
	iters: 900/1035, epoch: 17 | loss=0.141936 (base=0.137948, recon=0.121972, pred=0.159759, reg=0.019937, lambda_grad*reg=0.003987) | batch_x(mean/std)=-0.0538/0.9814 | speed=0.0752s/iter, left=10.2s
	iters: 1000/1035, epoch: 17 | loss=0.148679 (base=0.144583, recon=0.129580, pred=0.150026, reg=0.020478, lambda_grad*reg=0.004096) | batch_x(mean/std)=0.0363/0.9952 | speed=0.0751s/iter, left=2.7s
Epoch: 17 cost time: 85.1s | Train Loss: 0.164113 Vali Loss: 0.155430
Validation loss decreased (0.155461 --> 0.155430).  Saving model ...
Updating learning rate to 1.52587890625e-09
	iters: 100/1035, epoch: 18 | loss=0.139595 (base=0.135422, recon=0.119991, pred=0.154309, reg=0.020865, lambda_grad*reg=0.004173) | batch_x(mean/std)=0.0783/0.9725 | speed=0.1693s/iter, left=158.5s
	iters: 200/1035, epoch: 18 | loss=0.146474 (base=0.142496, recon=0.120842, pred=0.216537, reg=0.019892, lambda_grad*reg=0.003978) | batch_x(mean/std)=-0.0375/0.9888 | speed=0.0751s/iter, left=62.8s
	iters: 300/1035, epoch: 18 | loss=0.135142 (base=0.131012, recon=0.117458, pred=0.135542, reg=0.020648, lambda_grad*reg=0.004130) | batch_x(mean/std)=0.0495/0.9870 | speed=0.0753s/iter, left=55.4s
	iters: 400/1035, epoch: 18 | loss=0.148679 (base=0.144362, recon=0.124601, pred=0.197610, reg=0.021586, lambda_grad*reg=0.004317) | batch_x(mean/std)=0.2285/1.0582 | speed=0.0751s/iter, left=47.8s
	iters: 500/1035, epoch: 18 | loss=0.192437 (base=0.188415, recon=0.170344, pred=0.180706, reg=0.020111, lambda_grad*reg=0.004022) | batch_x(mean/std)=-0.0200/1.0782 | speed=0.0750s/iter, left=40.2s
	iters: 600/1035, epoch: 18 | loss=0.149114 (base=0.144998, recon=0.132146, pred=0.128526, reg=0.020579, lambda_grad*reg=0.004116) | batch_x(mean/std)=-0.1370/0.9874 | speed=0.0756s/iter, left=33.0s
	iters: 700/1035, epoch: 18 | loss=0.139137 (base=0.135346, recon=0.121595, pred=0.137509, reg=0.018952, lambda_grad*reg=0.003790) | batch_x(mean/std)=-0.0574/0.9267 | speed=0.0753s/iter, left=25.3s
	iters: 800/1035, epoch: 18 | loss=0.143167 (base=0.139373, recon=0.111760, pred=0.276128, reg=0.018973, lambda_grad*reg=0.003795) | batch_x(mean/std)=-0.0492/0.9377 | speed=0.0757s/iter, left=17.9s
	iters: 900/1035, epoch: 18 | loss=0.220028 (base=0.216226, recon=0.175414, pred=0.408119, reg=0.019010, lambda_grad*reg=0.003802) | batch_x(mean/std)=-0.0263/0.9419 | speed=0.0754s/iter, left=10.2s
	iters: 1000/1035, epoch: 18 | loss=0.162557 (base=0.158585, recon=0.144606, pred=0.139792, reg=0.019861, lambda_grad*reg=0.003972) | batch_x(mean/std)=-0.0479/1.0016 | speed=0.0754s/iter, left=2.7s
Epoch: 18 cost time: 84.9s | Train Loss: 0.164172 Vali Loss: 0.155590
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.62939453125e-10
	iters: 100/1035, epoch: 19 | loss=0.178241 (base=0.174377, recon=0.160483, pred=0.138943, reg=0.019320, lambda_grad*reg=0.003864) | batch_x(mean/std)=-0.0200/1.0134 | speed=0.1684s/iter, left=157.6s
	iters: 200/1035, epoch: 19 | loss=0.152549 (base=0.148622, recon=0.131447, pred=0.171759, reg=0.019632, lambda_grad*reg=0.003926) | batch_x(mean/std)=-0.0548/0.9218 | speed=0.0752s/iter, left=62.8s
	iters: 300/1035, epoch: 19 | loss=0.188151 (base=0.184205, recon=0.164467, pred=0.197385, reg=0.019728, lambda_grad*reg=0.003946) | batch_x(mean/std)=0.0236/1.0543 | speed=0.0753s/iter, left=55.4s
	iters: 400/1035, epoch: 19 | loss=0.128920 (base=0.125099, recon=0.110331, pred=0.147676, reg=0.019105, lambda_grad*reg=0.003821) | batch_x(mean/std)=0.0591/0.9660 | speed=0.0746s/iter, left=47.5s
	iters: 500/1035, epoch: 19 | loss=0.268709 (base=0.264697, recon=0.236678, pred=0.280188, reg=0.020064, lambda_grad*reg=0.004013) | batch_x(mean/std)=0.0487/1.0437 | speed=0.0755s/iter, left=40.5s
	iters: 600/1035, epoch: 19 | loss=0.124779 (base=0.120617, recon=0.105922, pred=0.146959, reg=0.020807, lambda_grad*reg=0.004161) | batch_x(mean/std)=0.0518/0.9706 | speed=0.0757s/iter, left=33.0s
	iters: 700/1035, epoch: 19 | loss=0.123271 (base=0.119550, recon=0.107119, pred=0.124309, reg=0.018608, lambda_grad*reg=0.003722) | batch_x(mean/std)=-0.0857/0.9046 | speed=0.0751s/iter, left=25.2s
	iters: 800/1035, epoch: 19 | loss=0.139588 (base=0.135387, recon=0.119542, pred=0.158450, reg=0.021001, lambda_grad*reg=0.004200) | batch_x(mean/std)=0.0233/1.0447 | speed=0.0755s/iter, left=17.8s
	iters: 900/1035, epoch: 19 | loss=0.119970 (base=0.116142, recon=0.101706, pred=0.144357, reg=0.019141, lambda_grad*reg=0.003828) | batch_x(mean/std)=-0.0949/0.9290 | speed=0.0756s/iter, left=10.3s
	iters: 1000/1035, epoch: 19 | loss=0.158183 (base=0.154356, recon=0.131529, pred=0.228267, reg=0.019136, lambda_grad*reg=0.003827) | batch_x(mean/std)=-0.0448/0.9744 | speed=0.0753s/iter, left=2.7s
Epoch: 19 cost time: 84.9s | Train Loss: 0.164079 Vali Loss: 0.155553
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.814697265625e-10
	iters: 100/1035, epoch: 20 | loss=0.129941 (base=0.126078, recon=0.112913, pred=0.131648, reg=0.019316, lambda_grad*reg=0.003863) | batch_x(mean/std)=-0.0054/0.9294 | speed=0.1669s/iter, left=156.2s
	iters: 200/1035, epoch: 20 | loss=0.123162 (base=0.119118, recon=0.106506, pred=0.126118, reg=0.020221, lambda_grad*reg=0.004044) | batch_x(mean/std)=-0.0259/0.9351 | speed=0.0752s/iter, left=62.9s
	iters: 300/1035, epoch: 20 | loss=0.193730 (base=0.189841, recon=0.173030, pred=0.168107, reg=0.019447, lambda_grad*reg=0.003889) | batch_x(mean/std)=-0.0798/1.0163 | speed=0.0756s/iter, left=55.6s
	iters: 400/1035, epoch: 20 | loss=0.140763 (base=0.136784, recon=0.123362, pred=0.134216, reg=0.019900, lambda_grad*reg=0.003980) | batch_x(mean/std)=0.0629/0.9779 | speed=0.0755s/iter, left=48.0s
	iters: 500/1035, epoch: 20 | loss=0.121683 (base=0.117661, recon=0.106247, pred=0.114142, reg=0.020112, lambda_grad*reg=0.004022) | batch_x(mean/std)=0.0742/1.0241 | speed=0.0755s/iter, left=40.5s
	iters: 600/1035, epoch: 20 | loss=0.155992 (base=0.151968, recon=0.135524, pred=0.164438, reg=0.020123, lambda_grad*reg=0.004025) | batch_x(mean/std)=-0.0459/1.0179 | speed=0.0756s/iter, left=32.9s
	iters: 700/1035, epoch: 20 | loss=0.122167 (base=0.118309, recon=0.103955, pred=0.143540, reg=0.019291, lambda_grad*reg=0.003858) | batch_x(mean/std)=0.0358/0.9414 | speed=0.0755s/iter, left=25.4s
	iters: 800/1035, epoch: 20 | loss=0.133811 (base=0.129680, recon=0.115574, pred=0.141059, reg=0.020656, lambda_grad*reg=0.004131) | batch_x(mean/std)=-0.0353/0.9723 | speed=0.0754s/iter, left=17.8s
	iters: 900/1035, epoch: 20 | loss=0.126476 (base=0.122660, recon=0.108695, pred=0.139654, reg=0.019078, lambda_grad*reg=0.003816) | batch_x(mean/std)=-0.0112/0.9003 | speed=0.0756s/iter, left=10.3s
	iters: 1000/1035, epoch: 20 | loss=0.185294 (base=0.181513, recon=0.153309, pred=0.282041, reg=0.018901, lambda_grad*reg=0.003780) | batch_x(mean/std)=0.0503/0.9691 | speed=0.0758s/iter, left=2.7s
Epoch: 20 cost time: 85.3s | Train Loss: 0.164223 Vali Loss: 0.155431
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : anomaly_detection_PSM_GOracleAD_G_OracleAD_PSM_ftM_sl100_ll48_pl0_dm128_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test: (87841, 25)
train: (132481, 25)
test 87742
test: (87841, 25)
train: (132481, 25)
train 132382
[Test-Debug][Test]  batch_x=(128, 100, 25) recon=(128, 100, 25) x(mean/std)=0.0979/1.0020 recon(mean/std)=0.2655/0.7567
[Test-Debug][Test]  batch_x=(128, 100, 25) recon=(128, 100, 25) x(mean/std)=0.1282/0.8081 recon(mean/std)=0.3016/0.0670
Threshold : 4.0420208024978646
pred:    (8774200,)
gt:      (8774200,)
pred:  (8774200,)
gt:    (8774200,)
Accuracy : 0.9483, Precision : 0.9783, Recall : 0.8322, F-score : 0.8993 
